# -*- coding: utf-8 -*-
"""Data Science Task_Rayhan Narawangsa

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IG8wlpTzGcmCG01VN9EthQkRw2B56YeW

# Import Data and Modules
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
import warnings
warnings.filterwarnings('ignore')

# Read the data and have a thorough understanding about each variables. 

data = pd.read_excel('Train.xlsx')
data.head(5)

"""# Exploratory Data Analysis"""

# we knew that the Train data is consist of 164309 rows. But before going further we have to check if there is any duplicate rows.  
# I am using Loan_ID because it is supposed to have unique characteristics.

data['Loan_ID'].nunique()
data['Loan_ID'].duplicated().value_counts()

# The number shown using nunique command shows the exacts number of rows in the Training data. Therefore, no duplicates in occured.

# Next step is to check if there is the data type that is not correct. 
# There might be some chances that we found a variables having object data type, when it should all be a numerical value, or vice-versa.

data.info()

# It shows that there is no uncorrect data type. However if there is any, we can use these two options to fix it:


# Let's say for the example, we found there area some objects value in Annual_salary variable. Then we can use option 1 or 2.  
# 1 ------ data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')
# 2 ------ data['TotalCharges'].replace(' ', np.nan, inplace=True)
#          data['TotalCharges'] = data['TotalCharges'].astype(float)

# Option 1 is used to converst argument into numerical data, with then invalid parsing will be set as NaN. Option 2 is used to replace blank values (white space) with NaN.

# After we check for duplicates and uncorrect data type, we also need to check if there is any null value on all of the variables.

data.isna().sum()

# We have found that there is some null values in multiple variable. We can this this later in preprocessing stages.

# Purpose of this stage is to bring the picture of your data is doing.

def category_unique_value():
    for cat_cols in (
        data.select_dtypes(exclude=[np.int64, np.float64]).columns.unique().to_list()
    ):
        print("Jumlah data Unique values dan koresponden untuk Field / feature: ")
        print("-" * 90)
        data_temp = pd.concat(
            [
                data[cat_cols].value_counts(),
                data[cat_cols].value_counts(normalize=True) * 100,
            ],
            axis=1,
        )
        data_temp.columns = ["Count", "Percentage"]
        print(data_temp)
        print("-" * 90)

category_unique_value()

list_columns = ['Length_Employed', 'Home_Owner', 'Income_Verified', 'Purpose_Of_Loan',]

for i in range(len(list_columns)):
  plt.figure(figsize=(22, 5))
  ax=sns.countplot(data=data, x=list_columns[i], hue='Interest_Rate', saturation=1, alpha=0.9, palette='flare')
  ax.set_title('Interest_Rate by '+list_columns[i])
  for p in ax.patches:
    ax.annotate(f'\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='white', size=10)

data_vis = data['Number_Open_Accounts']

import seaborn as sns


sns.pairplot(data_vis, hue='Interest_Rate', height=3)
plt.show()

# Charts above shows the big picture of the relationship between each variables and its tendency for the interest rate categories. 
# This helps me to justify which variables have an interisting/distinctive tendency for the interest categories.  
# Some variables are more interisting to have a closer look on: Number_Open_Accounts, Annual_Income, Total_Accounts, and Inquiries_Last_6Mo

# We can bring a closer look into some variables by using: 

list_columns = ['Number_Open_Accounts', 'Total_Accounts','Inquiries_Last_6Mo']

for i in range(len(list_columns)):
  plt.figure(figsize=(18,4))
  sns.histplot(data=data, x=list_columns[i], hue = 'Interest_Rate', palette='flare',binwidth=2 )
  plt.show()

# Variable Annual_Income have a very large distribution range. Therefore, we can use logarithmic chart.

plt2 = plt.figure(figsize=(18,4)) 
plt2 = sns.histplot(data=data, x='Annual_Income', log_scale=True, hue = 'Interest_Rate', palette= 'flare',binwidth=0.05)

display(data['Interest_Rate'].value_counts())
plt.figure(figsize=(8,7), facecolor='lightyellow')
plt.pie(data['Interest_Rate'].value_counts(), autopct='%.2f%%', pctdistance = 1.25,startangle=45, textprops={'fontsize': 15},
colors=['Purple','plum', 'darksalmon'], shadow=True)
my_circle=plt.Circle( (0,0), 0.6, color='lightyellow')
p=plt.gcf()
p.gca().add_artist(my_circle)
plt.title('Interest Rate Proportion', fontsize=17, fontweight='bold')
plt.legend(['1', '2', '3' ], bbox_to_anchor=(1, 1), fontsize=12)
plt.show()

"""# Data Preprocessing """

# Handling null data.

data.isna().sum()

plt.figure(figsize=(18,4)) 
sns.histplot(data=data, x='Annual_Income', log_scale=True, palette= 'Purple', binwidth=0.03, kde=True, color= 'Purple')

plt.figure(figsize=(18,4))
sns.histplot(data=data, x='Months_Since_Deliquency', palette='flare',binwidth=10, kde=True, color='Purple')

data2 = data

data2.describe().T

list_columns = ['Length_Employed', 'Home_Owner']

for i in range(len(list_columns)):
  plt.figure(figsize=(22, 5))
  ax=sns.countplot(data=data2, x=list_columns[i], saturation=1, alpha=0.9, palette='flare')
  ax.set_title('Interest_Rate by '+list_columns[i])
  for p in ax.patches:
    ax.annotate(f'\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='white', size=10)

# Mode of Length_Employed
mode = data2['Length_Employed'].mode()
print("The mode of Length_Employed: "+mode)

# Mode of Home_Owner
mode = data2['Home_Owner'].mode()
print("The mode of Home_Owner: "+mode)

# Mode of Customer_Age
median = data2['Annual_Income'].median()
print("The median of Annual_Income: {}".format(int(median)))

# Mode of Customer_Age 
median = data2['Months_Since_Deliquency'].median()
print("The median of Months_Since_Deliquency	: {}".format(int(median)))

data2 = data

data2['Length_Employed'] = data2['Length_Employed'].fillna('10+ years')
data2['Home_Owner'] = data2['Home_Owner'].fillna('Mortgage')
data2['Annual_Income'] = data2['Annual_Income'].fillna('63000')
data2['Months_Since_Deliquency'] = data2['Months_Since_Deliquency'].fillna('31')

data2.isna().sum()

data3 = data2

data3

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
lenc = LabelEncoder()
cols_lenc = ['Length_Employed']

# Label Encoding

for i in cols_lenc:
  lenc.fit(data3[i])
  data3[i] = lenc.transform(data3[i])

data3.head()

# One-Hot Encoding

!pip install category_encoders
import category_encoders as ce

cols_ohc = ['Home_Owner','Income_Verified', 'Gender', 'Purpose_Of_Loan']

ce_ohc = ce.OneHotEncoder(cols=['Home_Owner','Income_Verified', 'Gender', 'Purpose_Of_Loan'])

data3 = ce_ohc.fit_transform(data3)

data3

plt.figure(figsize=(20,20))
sns.heatmap(data3.corr(), annot=True)

# Choose Input and Output Category

data_x = data3[['Number_Open_Accounts', 'Total_Accounts', 'Debt_To_Income', 'Loan_Amount_Requested', 'Income_Verified_2'
               ,'Home_Owner_2','Annual_Income', 'Inquiries_Last_6Mo','Purpose_Of_Loan_4','Purpose_Of_Loan_2',
                'Purpose_Of_Loan_6']].copy()
data_y = data3['Interest_Rate']

from sklearn.model_selection import train_test_split
from collections import Counter

x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=5)
Counter(y_train)

# Scaling

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaled_train = np.array(x_train[['Number_Open_Accounts', 'Total_Accounts', 'Debt_To_Income', 'Loan_Amount_Requested', 'Income_Verified_2'
               ,'Home_Owner_2','Annual_Income', 'Inquiries_Last_6Mo','Purpose_Of_Loan_4','Purpose_Of_Loan_2',
                'Purpose_Of_Loan_6']]).reshape(-1,11)

scaler = scaler.fit(scaled_train)

x_train[['Number_Open_Accounts', 'Total_Accounts', 'Debt_To_Income', 'Loan_Amount_Requested', 'Income_Verified_2'
               ,'Home_Owner_2','Annual_Income', 'Inquiries_Last_6Mo','Purpose_Of_Loan_4','Purpose_Of_Loan_2',
                'Purpose_Of_Loan_6']] = scaler.transform(scaled_train)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

rf = RandomForestClassifier(random_state=5, criterion='entropy', n_estimators=18, max_depth=15)
rf.fit(x_train, y_train)
predictionrf = rf.predict(x_test)

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(random_state = 100)
lr.fit(x_train, y_train)
predictionlr = lr.predict(x_test)

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

predictionsvm = SVC()
predictionsvm.fit(x_train, y_train)

print("Akurasi dari SVM adalah: %.2f" % (accuracy_score(y_test, predictionsvm)*100) )

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
knn.fit(x_train, y_train)

KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,
                     weights='uniform')
predictionknn = knn.predict(x_test)

from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
gnb.fit(x_train, y_train)

predictiongnb = gnb.predict(x_test)

from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier(random_state=5, learning_rate=1, n_estimators=50)
ada.fit(x_train, y_train)
predictionada = ada.predict(x_test)

# Hasil
print("Akurasi dari Random Forest adalah: %.2f" % (accuracy_score(y_test, predictionrf)*100) )
print("Akurasi dari Logistic Regression adalah: %.2f" % (accuracy_score(y_test, predictionlr)*100) )
print("Akurasi dari K-Nearest Neighbors adalah: %.2f" % (accuracy_score(y_test, predictionknn)*100) )
print("Akurasi dari Naive Bays adalah: %.2f" % (accuracy_score(y_test, predictiongnb)*100) )
print("Akurasi dari Ada Boost Classifier adalah: %.2f" % (accuracy_score(y_test, predictionada)*100) )

# Hasil Setelah Scaler
print("Akurasi dari Random Forest adalah: %.2f" % (accuracy_score(y_test, predictionrf)*100) )
print("Akurasi dari Naive Bays adalah: %.2f" % (accuracy_score(y_test, predictiongnb)*100) )
print("Akurasi dari Ada Boost Classifier adalah: %.2f" % (accuracy_score(y_test, predictionada)*100) )

#SMOTE

!pip install imbalanced-learn
from imblearn.over_sampling import SMOTE

sm = SMOTE(sampling_strategy = 'auto', k_neighbors=5, random_state=5)
x_resample, y_resample = sm.fit_resample(x_train, y_train)

rf.fit(x_resample, y_resample)
predictionrfsm = rf.predict(x_test)

ada = AdaBoostClassifier(random_state=5, learning_rate=0.5, n_estimators=50)
ada.fit(x_resample, y_resample)
predictionadasm = ada.predict(x_test)

gnb = GaussianNB()
gnb.fit(x_resample, y_resample)

predictiongnbsm = gnb.predict(x_test)

# Hasil Setelah Scaler dan Smote
print("Akurasi dari Random Forest adalah: %.2f" % (accuracy_score(y_test, predictionrfsm)*100) )
print("Akurasi dari Naive Bays adalah: %.2f" % (accuracy_score(y_test, predictiongnbsm)*100) )
print("Akurasi dari Ada Boost Classifier adalah: %.2f" % (accuracy_score(y_test, predictionadasm)*100) )

lr.fit(x_resample, y_resample)
predictionlrsm = lr.predict(x_test)

predictionsvmsm = SVC()
predictionsvmsm.fit(x_resample, y_resample)

knn.fit(x_resample, y_resample)

KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,
                     weights='uniform')
predictionknnsm = knn.predict(x_test)

print("Akurasi dari Random Forest adalah: %.2f" % (accuracy_score(y_test, predictionrfsm)*100) )
print("Akurasi dari Logistic Regression adalah: %.2f" % (accuracy_score(y_test, predictionlrsm)*100) )
print("Akurasi dari K-Nearest Neighbors adalah: %.2f" % (accuracy_score(y_test, predictionknnsm)*100) )
print("Akurasi dari Naive Bays adalah: %.2f" % (accuracy_score(y_test, predictiongnbsm)*100) )
print("Akurasi dari Ada Boost Classifier adalah: %.2f" % (accuracy_score(y_test, predictionadasm)*100) )

lgbm = LGBMClassifier(random_state=5, learning_rate= 0.05, n_estimators= 90, num_leaves= 20, boosting_type='dart')
lgbm.fit(x_resample, y_resample)
predictionlgbmsm = lgbm.predict(x_test)

print("Akurasi dari Light GBM adalah: %.2f" % (accuracy_score(y_test, predictionlgbmsm)*100) )